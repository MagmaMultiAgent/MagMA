{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'importlib_metadata' from '/Users/ranuon/miniforge3/envs/luxai_s2/lib/python3.8/site-packages/importlib_metadata/__init__.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import importlib_metadata\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "importlib.reload(importlib_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "# Controller class copied here since you won't have access to the luxai_s2 package directly on the competition server\n",
    "class Controller:\n",
    "    def __init__(self, action_space: spaces.Space) -> None:\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def action_to_lux_action(\n",
    "        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input the current \"raw observation\" and the parameterized action and returns\n",
    "        an action formatted for the Lux env\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def action_masks(self, agent: str, obs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Generates a boolean action mask indicating in each discrete dimension whether it would be valid or not\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SimpleUnitDiscreteController(Controller):\n",
    "    def __init__(self, env_cfg) -> None:\n",
    "        \"\"\"\n",
    "        A simple controller that controls only the robot that will get spawned.\n",
    "        Moreover, it will always try to spawn one heavy robot if there are none regardless of action given\n",
    "\n",
    "        For the robot unit\n",
    "        - 4 cardinal direction movement (4 dims)\n",
    "        - a move center no-op action (1 dim)\n",
    "        - transfer action just for transferring ice in 4 cardinal directions or center (5)\n",
    "        - pickup action for power (1 dims)\n",
    "        - dig action (1 dim)\n",
    "        - no op action (1 dim) - equivalent to not submitting an action queue which costs power\n",
    "        It does not include\n",
    "        - self destruct action\n",
    "        - recharge action\n",
    "        - planning (via actions executing multiple times or repeating actions)\n",
    "        - factory actions\n",
    "        - transferring power or resources other than ice\n",
    "\n",
    "        To help understand how to this controller works to map one action space to the original lux action space,\n",
    "        see how the lux action space is defined in luxai_s2/spaces/action.py\n",
    "\n",
    "        \"\"\"\n",
    "        self.env_cfg = env_cfg\n",
    "        self.move_act_dims = 4\n",
    "        self.transfer_act_dims = 5\n",
    "        self.pickup_act_dims = 1\n",
    "        self.dig_act_dims = 1\n",
    "        self.no_op_dims = 1\n",
    "\n",
    "        self.move_dim_high = self.move_act_dims\n",
    "        self.transfer_dim_high = self.move_dim_high + self.transfer_act_dims\n",
    "        self.pickup_dim_high = self.transfer_dim_high + self.pickup_act_dims\n",
    "        self.dig_dim_high = self.pickup_dim_high + self.dig_act_dims\n",
    "        self.no_op_dim_high = self.dig_dim_high + self.no_op_dims\n",
    "\n",
    "        self.total_act_dims = self.no_op_dim_high\n",
    "        action_space = spaces.Discrete(self.total_act_dims)\n",
    "        super().__init__(action_space)\n",
    "\n",
    "    def _is_move_action(self, id):\n",
    "        return id < self.move_dim_high\n",
    "\n",
    "    def _get_move_action(self, id):\n",
    "        # move direction is id + 1 since we don't allow move center here\n",
    "        return np.array([0, id + 1, 0, 0, 0, 1])\n",
    "\n",
    "    def _is_transfer_action(self, id):\n",
    "        return id < self.transfer_dim_high\n",
    "\n",
    "    def _get_transfer_action(self, id):\n",
    "        id = id - self.move_dim_high\n",
    "        transfer_dir = id % 5\n",
    "        return np.array([1, transfer_dir, 0, self.env_cfg.max_transfer_amount, 0, 1])\n",
    "\n",
    "    def _is_pickup_action(self, id):\n",
    "        return id < self.pickup_dim_high\n",
    "\n",
    "    def _get_pickup_action(self, id):\n",
    "        return np.array([2, 0, 4, self.env_cfg.max_transfer_amount, 0, 1])\n",
    "\n",
    "    def _is_dig_action(self, id):\n",
    "        return id < self.dig_dim_high\n",
    "\n",
    "    def _get_dig_action(self, id):\n",
    "        return np.array([3, 0, 0, 0, 0, 1])\n",
    "\n",
    "    def action_to_lux_action(\n",
    "        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n",
    "    ):\n",
    "        shared_obs = obs[\"player_0\"]\n",
    "        lux_action = dict()\n",
    "        units = shared_obs[\"units\"][agent]\n",
    "        for unit_id in units.keys():\n",
    "            unit = units[unit_id]\n",
    "            choice = action\n",
    "            action_queue = []\n",
    "            no_op = False\n",
    "            if self._is_move_action(choice):\n",
    "                action_queue = [self._get_move_action(choice)]\n",
    "            elif self._is_transfer_action(choice):\n",
    "                action_queue = [self._get_transfer_action(choice)]\n",
    "            elif self._is_pickup_action(choice):\n",
    "                action_queue = [self._get_pickup_action(choice)]\n",
    "            elif self._is_dig_action(choice):\n",
    "                action_queue = [self._get_dig_action(choice)]\n",
    "            else:\n",
    "                # action is a no_op, so we don't update the action queue\n",
    "                no_op = True\n",
    "\n",
    "            # simple trick to help agents conserve power is to avoid updating the action queue\n",
    "            # if the agent was previously trying to do that particular action already\n",
    "            if len(unit[\"action_queue\"]) > 0 and len(action_queue) > 0:\n",
    "                same_actions = (unit[\"action_queue\"][0] == action_queue[0]).all()\n",
    "                if same_actions:\n",
    "                    no_op = True\n",
    "            if not no_op:\n",
    "                lux_action[unit_id] = action_queue\n",
    "\n",
    "            break\n",
    "        factories = shared_obs[\"factories\"][agent]\n",
    "        if len(units) == 0:\n",
    "            for unit_id in factories.keys():\n",
    "                lux_action[unit_id] = 1  # build a single heavy\n",
    "\n",
    "        return lux_action\n",
    "\n",
    "    def action_masks(self, agent: str, obs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Defines a simplified action mask for this controller's action space\n",
    "\n",
    "        Doesn't account for whether robot has enough power\n",
    "        \"\"\"\n",
    "\n",
    "        # compute a factory occupancy map that will be useful for checking if a board tile\n",
    "        # has a factory and which team's factory it is.\n",
    "        shared_obs = obs[agent]\n",
    "        factory_occupancy_map = (\n",
    "            np.ones_like(shared_obs[\"board\"][\"rubble\"], dtype=int) * -1\n",
    "        )\n",
    "        factories = dict()\n",
    "        for player in shared_obs[\"factories\"]:\n",
    "            factories[player] = dict()\n",
    "            for unit_id in shared_obs[\"factories\"][player]:\n",
    "                f_data = shared_obs[\"factories\"][player][unit_id]\n",
    "                f_pos = f_data[\"pos\"]\n",
    "                # store in a 3x3 space around the factory position it's strain id.\n",
    "                factory_occupancy_map[\n",
    "                    f_pos[0] - 1 : f_pos[0] + 2, f_pos[1] - 1 : f_pos[1] + 2\n",
    "                ] = f_data[\"strain_id\"]\n",
    "import sys\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "# Controller class copied here since you won't have access to the luxai_s2 package directly on the competition server\n",
    "class Controller:\n",
    "    def __init__(self, action_space: spaces.Space) -> None:\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def action_to_lux_action(\n",
    "        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input the current \"raw observation\" and the parameterized action and returns\n",
    "        an action formatted for the Lux env\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def action_masks(self, agent: str, obs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Generates a boolean action mask indicating in each discrete dimension whether it would be valid or not\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SimpleUnitDiscreteController(Controller):\n",
    "    def __init__(self, env_cfg) -> None:\n",
    "        \"\"\"\n",
    "        A simple controller that controls only the robot that will get spawned.\n",
    "        Moreover, it will always try to spawn one heavy robot if there are none regardless of action given\n",
    "\n",
    "        For the robot unit\n",
    "        - 4 cardinal direction movement (4 dims)\n",
    "        - a move center no-op action (1 dim)\n",
    "        - transfer action just for transferring ice in 4 cardinal directions or center (5)\n",
    "        - pickup action for power (1 dims)\n",
    "        - dig action (1 dim)\n",
    "        - no op action (1 dim) - equivalent to not submitting an action queue which costs power\n",
    "\n",
    "        It does not include\n",
    "        - self destruct action\n",
    "        - recharge action\n",
    "        - planning (via actions executing multiple times or repeating actions)\n",
    "        - factory actions\n",
    "        - transferring power or resources other than ice\n",
    "\n",
    "        To help understand how to this controller works to map one action space to the original lux action space,\n",
    "        see how the lux action space is defined in luxai_s2/spaces/action.py\n",
    "\n",
    "        \"\"\"\n",
    "        self.env_cfg = env_cfg\n",
    "        self.move_act_dims = 4\n",
    "        self.transfer_act_dims = 5\n",
    "        self.pickup_act_dims = 1\n",
    "        self.dig_act_dims = 1\n",
    "        self.no_op_dims = 1\n",
    "\n",
    "        self.move_dim_high = self.move_act_dims\n",
    "        self.transfer_dim_high = self.move_dim_high + self.transfer_act_dims\n",
    "        self.pickup_dim_high = self.transfer_dim_high + self.pickup_act_dims\n",
    "        self.dig_dim_high = self.pickup_dim_high + self.dig_act_dims\n",
    "        self.no_op_dim_high = self.dig_dim_high + self.no_op_dims\n",
    "\n",
    "        self.total_act_dims = self.no_op_dim_high\n",
    "        action_space = spaces.Discrete(self.total_act_dims)\n",
    "        super().__init__(action_space)\n",
    "\n",
    "    def _is_move_action(self, id):\n",
    "        return id < self.move_dim_high\n",
    "\n",
    "    def _get_move_action(self, id):\n",
    "        # move direction is id + 1 since we don't allow move center here\n",
    "        return np.array([0, id + 1, 0, 0, 0, 1])\n",
    "\n",
    "    def _is_transfer_action(self, id):\n",
    "        return id < self.transfer_dim_high\n",
    "\n",
    "    def _get_transfer_action(self, id):\n",
    "        id = id - self.move_dim_high\n",
    "        transfer_dir = id % 5\n",
    "        return np.array([1, transfer_dir, 0, self.env_cfg.max_transfer_amount, 0, 1])\n",
    "\n",
    "    def _is_pickup_action(self, id):\n",
    "        return id < self.pickup_dim_high\n",
    "\n",
    "    def _get_pickup_action(self, id):\n",
    "        return np.array([2, 0, 4, self.env_cfg.max_transfer_amount, 0, 1])\n",
    "\n",
    "    def _is_dig_action(self, id):\n",
    "        return id < self.dig_dim_high\n",
    "\n",
    "    def _get_dig_action(self, id):\n",
    "        return np.array([3, 0, 0, 0, 0, 1])\n",
    "\n",
    "    def action_to_lux_action(\n",
    "        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n",
    "    ):\n",
    "        shared_obs = obs[\"player_0\"]\n",
    "        lux_action = dict()\n",
    "        units = shared_obs[\"units\"][agent]\n",
    "        for unit_id in units.keys():\n",
    "            unit = units[unit_id]\n",
    "            choice = action\n",
    "            action_queue = []\n",
    "            no_op = False\n",
    "            if self._is_move_action(choice):\n",
    "                action_queue = [self._get_move_action(choice)]\n",
    "            elif self._is_transfer_action(choice):\n",
    "                action_queue = [self._get_transfer_action(choice)]\n",
    "            elif self._is_pickup_action(choice):\n",
    "                action_queue = [self._get_pickup_action(choice)]\n",
    "            elif self._is_dig_action(choice):\n",
    "                action_queue = [self._get_dig_action(choice)]\n",
    "            else:\n",
    "                # action is a no_op, so we don't update the action queue\n",
    "                no_op = True\n",
    "\n",
    "            # simple trick to help agents conserve power is to avoid updating the action queue\n",
    "            # if the agent was previously trying to do that particular action already\n",
    "            if len(unit[\"action_queue\"]) > 0 and len(action_queue) > 0:\n",
    "                same_actions = (unit[\"action_queue\"][0] == action_queue[0]).all()\n",
    "                if same_actions:\n",
    "                    no_op = True\n",
    "            if not no_op:\n",
    "                lux_action[unit_id] = action_queue\n",
    "\n",
    "            break\n",
    "\n",
    "        factories = shared_obs[\"factories\"][agent]\n",
    "        if len(units) == 0:\n",
    "            for unit_id in factories.keys():\n",
    "                lux_action[unit_id] = 1  # build a single heavy\n",
    "\n",
    "        return lux_action\n",
    "\n",
    "    def action_masks(self, agent: str, obs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Defines a simplified action mask for this controller's action space\n",
    "\n",
    "        Doesn't account for whether robot has enough power\n",
    "        \"\"\"\n",
    "\n",
    "        # compute a factory occupancy map that will be useful for checking if a board tile\n",
    "        # has a factory and which team's factory it is.\n",
    "        shared_obs = obs[agent]\n",
    "        factory_occupancy_map = (\n",
    "            np.ones_like(shared_obs[\"board\"][\"rubble\"], dtype=int) * -1\n",
    "        )\n",
    "        factories = dict()\n",
    "        for player in shared_obs[\"factories\"]:\n",
    "            factories[player] = dict()\n",
    "            for unit_id in shared_obs[\"factories\"][player]:\n",
    "                f_data = shared_obs[\"factories\"][player][unit_id]\n",
    "                f_pos = f_data[\"pos\"]\n",
    "                # store in a 3x3 space around the factory position it's strain id.\n",
    "                factory_occupancy_map[\n",
    "                    f_pos[0] - 1 : f_pos[0] + 2, f_pos[1] - 1 : f_pos[1] + 2\n",
    "                ] = f_data[\"strain_id\"]\n",
    "\n",
    "        units = shared_obs[\"units\"][agent]\n",
    "        action_mask = np.zeros((self.total_act_dims), dtype=bool)\n",
    "        for unit_id in units.keys():\n",
    "            action_mask = np.zeros(self.total_act_dims)\n",
    "            # movement is always valid\n",
    "            action_mask[:4] = True\n",
    "\n",
    "            # transferring is valid only if the target exists\n",
    "            unit = units[unit_id]\n",
    "            pos = np.array(unit[\"pos\"])\n",
    "            # a[1] = direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)\n",
    "            move_deltas = np.array([[0, 0], [0, -1], [1, 0], [0, 1], [-1, 0]])\n",
    "            for i, move_delta in enumerate(move_deltas):\n",
    "                transfer_pos = np.array(\n",
    "                    [pos[0] + move_delta[0], pos[1] + move_delta[1]]\n",
    "                )\n",
    "                # check if theres a factory tile there\n",
    "                if (\n",
    "                    transfer_pos[0] < 0\n",
    "                    or transfer_pos[1] < 0\n",
    "                    or transfer_pos[0] >= len(factory_occupancy_map)\n",
    "                    or transfer_pos[1] >= len(factory_occupancy_map[0])\n",
    "                ):\n",
    "                    continue\n",
    "                factory_there = factory_occupancy_map[transfer_pos[0], transfer_pos[1]]\n",
    "                if factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]:\n",
    "                    action_mask[\n",
    "                        self.transfer_dim_high - self.transfer_act_dims + i\n",
    "                    ] = True\n",
    "            factory_there = factory_occupancy_map[pos[0], pos[1]]\n",
    "            on_top_of_factory = (\n",
    "                factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]\n",
    "            )\n",
    "\n",
    "            # dig is valid only if on top of tile with rubble or resources or lichen\n",
    "            board_sum = (\n",
    "                shared_obs[\"board\"][\"ice\"][pos[0], pos[1]]\n",
    "                + shared_obs[\"board\"][\"ore\"][pos[0], pos[1]]\n",
    "                + shared_obs[\"board\"][\"rubble\"][pos[0], pos[1]]\n",
    "                + shared_obs[\"board\"][\"lichen\"][pos[0], pos[1]]\n",
    "            )\n",
    "            if board_sum > 0 and not on_top_of_factory:\n",
    "                action_mask[\n",
    "                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n",
    "                ] = True\n",
    "\n",
    "            # pickup is valid only if on top of factory tile\n",
    "            if on_top_of_factory:\n",
    "                action_mask[\n",
    "                    self.pickup_dim_high - self.pickup_act_dims : self.pickup_dim_high\n",
    "                ] = True\n",
    "                action_mask[\n",
    "                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n",
    "                ] = False\n",
    "\n",
    "            # no-op is always valid\n",
    "            action_mask[-1] = True\n",
    "            break\n",
    "        return action_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class SimpleUnitObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    A simple state based observation to work with in pair with the SimpleUnitDiscreteController\n",
    "\n",
    "    It contains info only on the first robot, the first factory you own, and some useful features. If there are no owned robots the observation is just zero.\n",
    "    No information about the opponent is included. This will generate observations for all teams.\n",
    "\n",
    "    Included features:\n",
    "    - First robot's stats\n",
    "    - distance vector to closest ice tile\n",
    "    - distance vector to first factory\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.observation_space = spaces.Box(-999, 999, shape=(13,))\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return SimpleUnitObservationWrapper.convert_obs(obs, self.env.state.env_cfg)\n",
    "\n",
    "    # we make this method static so the submission/evaluation code can use this as well\n",
    "    @staticmethod\n",
    "    def convert_obs(obs: Dict[str, Any], env_cfg: Any) -> Dict[str, npt.NDArray]:\n",
    "        observation = dict()\n",
    "        shared_obs = obs[\"player_0\"]\n",
    "        ice_map = shared_obs[\"board\"][\"ice\"]\n",
    "        ice_tile_locations = np.argwhere(ice_map == 1)\n",
    "\n",
    "        for agent in obs.keys():\n",
    "            obs_vec = np.zeros(\n",
    "                13,\n",
    "            )\n",
    "\n",
    "            factories = shared_obs[\"factories\"][agent]\n",
    "            factory_vec = np.zeros(2)\n",
    "            for k in factories.keys():\n",
    "                # here we track a normalized position of the first friendly factory\n",
    "                factory = factories[k]\n",
    "                factory_vec = np.array(factory[\"pos\"]) / env_cfg.map_size\n",
    "                break\n",
    "            units = shared_obs[\"units\"][agent]\n",
    "            for k in units.keys():\n",
    "                unit = units[k]\n",
    "\n",
    "                # store cargo+power values scaled to [0, 1]\n",
    "                cargo_space = env_cfg.ROBOTS[unit[\"unit_type\"]].CARGO_SPACE\n",
    "                battery_cap = env_cfg.ROBOTS[unit[\"unit_type\"]].BATTERY_CAPACITY\n",
    "                cargo_vec = np.array(\n",
    "                    [\n",
    "                        unit[\"power\"] / battery_cap,\n",
    "                        unit[\"cargo\"][\"ice\"] / cargo_space,\n",
    "                        unit[\"cargo\"][\"ore\"] / cargo_space,\n",
    "                        unit[\"cargo\"][\"water\"] / cargo_space,\n",
    "                        unit[\"cargo\"][\"metal\"] / cargo_space,\n",
    "                    ]\n",
    "                )\n",
    "                unit_type = (\n",
    "                    0 if unit[\"unit_type\"] == \"LIGHT\" else 1\n",
    "                )  # note that build actions use 0 to encode Light\n",
    "                # normalize the unit position\n",
    "                pos = np.array(unit[\"pos\"]) / env_cfg.map_size\n",
    "                unit_vec = np.concatenate(\n",
    "                    [pos, [unit_type], cargo_vec, [unit[\"team_id\"]]], axis=-1\n",
    "                )\n",
    "\n",
    "                # we add some engineered features down here\n",
    "                # compute closest ice tile\n",
    "                ice_tile_distances = np.mean(\n",
    "                    (ice_tile_locations - np.array(unit[\"pos\"])) ** 2, 1\n",
    "                )\n",
    "                # normalize the ice tile location\n",
    "                closest_ice_tile = (\n",
    "                    ice_tile_locations[np.argmin(ice_tile_distances)] / env_cfg.map_size\n",
    "                )\n",
    "                obs_vec = np.concatenate(\n",
    "                    [unit_vec, factory_vec - pos, closest_ice_tile - pos], axis=-1\n",
    "                )\n",
    "                break\n",
    "            observation[agent] = obs_vec\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "import luxai_s2.env\n",
    "from luxai_s2.env import LuxAI_S2\n",
    "from luxai_s2.state import ObservationStateDict\n",
    "from luxai_s2.unit import ActionType, BidActionType, FactoryPlacementActionType\n",
    "from luxai_s2.utils import my_turn_to_place_factory\n",
    "from luxai_s2.wrappers.controllers import (\n",
    "    Controller,\n",
    ")\n",
    "\n",
    "\n",
    "class SB3Wrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: LuxAI_S2,\n",
    "        bid_policy: Callable[\n",
    "            [str, ObservationStateDict], Dict[str, BidActionType]\n",
    "        ] = None,\n",
    "        factory_placement_policy: Callable[\n",
    "            [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n",
    "        ] = None,\n",
    "        controller: Controller = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A environment wrapper for Stable Baselines 3. It reduces the LuxAI_S2 env\n",
    "        into a single phase game and places the first two phases (bidding and factory placement) into the env.reset function so that\n",
    "        interacting agents directly start generating actions to play the third phase of the game.\n",
    "\n",
    "        It also accepts a Controller that translates action's in one action space to a Lux S2 compatible action\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bid_policy: Function\n",
    "            A function accepting player: str and obs: ObservationStateDict as input that returns a bid action\n",
    "            such as dict(bid=10, faction=\"AlphaStrike\"). By default will bid 0\n",
    "        factory_placement_policy: Function\n",
    "            A function accepting player: str and obs: ObservationStateDict as input that returns a factory placement action\n",
    "            such as dict(spawn=np.array([2, 4]), metal=150, water=150). By default will spawn in a random valid location with metal=150, water=150\n",
    "        controller : Controller\n",
    "            A controller that parameterizes the action space into something more usable and converts parameterized actions to lux actions.\n",
    "            See luxai_s2/wrappers/controllers.py for available controllers and how to make your own\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        \n",
    "        assert controller is not None\n",
    "        \n",
    "        # set our controller and replace the action space\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "import luxai_s2.env\n",
    "from luxai_s2.env import LuxAI_S2\n",
    "from luxai_s2.state import ObservationStateDict\n",
    "from luxai_s2.unit import ActionType, BidActionType, FactoryPlacementActionType\n",
    "from luxai_s2.utils import my_turn_to_place_factory\n",
    "from luxai_s2.wrappers.controllers import (\n",
    "    Controller,\n",
    ")\n",
    "\n",
    "\n",
    "class SB3Wrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: LuxAI_S2,\n",
    "        bid_policy: Callable[\n",
    "            [str, ObservationStateDict], Dict[str, BidActionType]\n",
    "        ] = None,\n",
    "        factory_placement_policy: Callable[\n",
    "            [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n",
    "        ] = None,\n",
    "        controller: Controller = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A environment wrapper for Stable Baselines 3. It reduces the LuxAI_S2 env\n",
    "        into a single phase game and places the first two phases (bidding and factory placement) into the env.reset function so that\n",
    "        interacting agents directly start generating actions to play the third phase of the game.\n",
    "\n",
    "        It also accepts a Controller that translates action's in one action space to a Lux S2 compatible action\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bid_policy: Function\n",
    "            A function accepting player: str and obs: ObservationStateDict as input that returns a bid action\n",
    "            such as dict(bid=10, faction=\"AlphaStrike\"). By default will bid 0\n",
    "        factory_placement_policy: Function\n",
    "            A function accepting player: str and obs: ObservationStateDict as input that returns a factory placement action\n",
    "            such as dict(spawn=np.array([2, 4]), metal=150, water=150). By default will spawn in a random valid location with metal=150, water=150\n",
    "        controller : Controller\n",
    "            A controller that parameterizes the action space into something more usable and converts parameterized actions to lux actions.\n",
    "            See luxai_s2/wrappers/controllers.py for available controllers and how to make your own\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        \n",
    "        assert controller is not None\n",
    "        \n",
    "        # set our controller and replace the action space\n",
    "        self.controller = controller\n",
    "        self.action_space = controller.action_space\n",
    "\n",
    "        # The simplified wrapper removes the first two phases of the game by using predefined policies (trained or heuristic)\n",
    "        # to handle those two phases during each reset\n",
    "        if factory_placement_policy is None:\n",
    "            def factory_placement_policy(player, obs: ObservationStateDict):\n",
    "                potential_spawns = np.array(\n",
    "                    list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n",
    "                )\n",
    "                spawn_loc = potential_spawns[\n",
    "                    np.random.randint(0, len(potential_spawns))\n",
    "                ]\n",
    "                return dict(spawn=spawn_loc, metal=150, water=150)\n",
    "\n",
    "        self.factory_placement_policy = factory_placement_policy\n",
    "        if bid_policy is None:\n",
    "            def bid_policy(player, obs: ObservationStateDict):\n",
    "                faction = \"AlphaStrike\"\n",
    "                if player == \"player_1\":\n",
    "                    faction = \"MotherMars\"\n",
    "                return dict(bid=0, faction=faction)\n",
    "\n",
    "        self.bid_policy = bid_policy\n",
    "\n",
    "        self.prev_obs = None\n",
    "\n",
    "    def step(self, action: Dict[str, npt.NDArray]):\n",
    "        # here, for each agent in the game we translate their action into a Lux S2 action\n",
    "        lux_action = dict()\n",
    "        for agent in self.env.agents:\n",
    "            if agent in action:\n",
    "                lux_action[agent] = self.controller.action_to_lux_action(\n",
    "                    agent=agent, obs=self.prev_obs, action=action[agent]\n",
    "                )\n",
    "            else:\n",
    "                lux_action[agent] = dict()\n",
    "        \n",
    "        # lux_action is now a dict mapping agent name to an action\n",
    "        obs, reward, done, info = self.env.step(lux_action)\n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # we upgrade the reset function here\n",
    "        \n",
    "        # we call the original reset function first\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        \n",
    "        # then use the bid policy to go through the bidding phase\n",
    "        action = dict()\n",
    "        for agent in self.env.agents:\n",
    "            action[agent] = self.bid_policy(agent, obs[agent])\n",
    "        obs, _, _, _ = self.env.step(action)\n",
    "        \n",
    "        # while real_env_steps < 0, we are in the factory placement phase\n",
    "        # so we use the factory placement policy to step through this\n",
    "        while self.env.state.real_env_steps < 0:\n",
    "            action = dict()\n",
    "            for agent in self.env.agents:\n",
    "                if my_turn_to_place_factory(\n",
    "                    obs[\"player_0\"][\"teams\"][agent][\"place_first\"],\n",
    "                    self.env.state.env_steps,\n",
    "                ):\n",
    "                    action[agent] = self.factory_placement_policy(agent, obs[agent])\n",
    "                else:\n",
    "                    action[agent] = dict()\n",
    "            obs, _, _, _ = self.env.step(action)\n",
    "        self.prev_obs = obs\n",
    "        \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_bid(player, obs):\n",
    "    # a policy that always bids 0\n",
    "    faction = \"AlphaStrike\"\n",
    "    if player == \"player_1\":\n",
    "        faction = \"MotherMars\"\n",
    "    return dict(bid=0, faction=faction)\n",
    "\n",
    "def place_near_random_ice(player, obs):\n",
    "    \"\"\"\n",
    "    This policy will place a single factory with all the starting resources\n",
    "    near a random ice tile\n",
    "    \"\"\"\n",
    "    if obs[\"teams\"][player][\"metal\"] == 0:\n",
    "        return dict()\n",
    "    potential_spawns = list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n",
    "    potential_spawns_set = set(potential_spawns)\n",
    "    done_search = False\n",
    "    \n",
    "    # simple numpy trick to find locations adjacent to ice tiles.\n",
    "    ice_diff = np.diff(obs[\"board\"][\"ice\"])\n",
    "    pot_ice_spots = np.argwhere(ice_diff == 1)\n",
    "    if len(pot_ice_spots) == 0:\n",
    "        pot_ice_spots = potential_spawns\n",
    "    # pick a random ice spot and search around it for spawnable locations.\n",
    "    trials = 5\n",
    "    while trials > 0:\n",
    "        pos_idx = np.random.randint(0, len(pot_ice_spots))\n",
    "        pos = pot_ice_spots[pos_idx]\n",
    "        area = 3\n",
    "        for x in range(area):\n",
    "            for y in range(area):\n",
    "                check_pos = [pos[0] + x - area // 2, pos[1] + y - area // 2]\n",
    "                if tuple(check_pos) in potential_spawns_set:\n",
    "                    done_search = True\n",
    "                    pos = check_pos\n",
    "                    break\n",
    "            if done_search:\n",
    "                break\n",
    "        if done_search:\n",
    "            break\n",
    "        trials -= 1\n",
    "    \n",
    "    if not done_search:\n",
    "        spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n",
    "        pos = spawn_loc\n",
    "    \n",
    "    # this will spawn a factory at pos and with all the starting metal and water\n",
    "    metal = obs[\"teams\"][player][\"metal\"]\n",
    "    return dict(spawn=pos, metal=metal, water=metal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "action space does not inherit from `gym.spaces.Space`, actual type: <class 'method'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mLuxAI_S2-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m env \u001b[39m=\u001b[39m SB3Wrapper(env, zero_bid, place_near_random_ice, controller\u001b[39m=\u001b[39mSimpleUnitDiscreteController(env\u001b[39m.\u001b[39menv_cfg))\n\u001b[1;32m      4\u001b[0m env\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39m\u001b[39m9999\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/luxai_s2/lib/python3.8/site-packages/gym/envs/registration.py:669\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[39m# Run the environment checker as the lowest level wrapper\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m disable_env_checker \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    667\u001b[0m     disable_env_checker \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m spec_\u001b[39m.\u001b[39mdisable_env_checker \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m ):\n\u001b[0;32m--> 669\u001b[0m     env \u001b[39m=\u001b[39m PassiveEnvChecker(env)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Add the order enforcing wrapper\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m spec_\u001b[39m.\u001b[39morder_enforce:\n",
      "File \u001b[0;32m~/miniforge3/envs/luxai_s2/lib/python3.8/site-packages/gym/wrappers/env_checker.py:23\u001b[0m, in \u001b[0;36mPassiveEnvChecker.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(env)\n\u001b[1;32m     20\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[1;32m     21\u001b[0m     env, \u001b[39m\"\u001b[39m\u001b[39maction_space\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mThe environment must specify an action space. https://www.gymlibrary.dev/content/environment_creation/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m check_action_space(env\u001b[39m.\u001b[39;49maction_space)\n\u001b[1;32m     24\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[1;32m     25\u001b[0m     env, \u001b[39m\"\u001b[39m\u001b[39mobservation_space\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mThe environment must specify an observation space. https://www.gymlibrary.dev/content/environment_creation/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m check_observation_space(env\u001b[39m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m~/miniforge3/envs/luxai_s2/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:74\u001b[0m, in \u001b[0;36mcheck_space\u001b[0;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check of the environment action space that should not affect the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(space, spaces\u001b[39m.\u001b[39mSpace):\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mspace_type\u001b[39m}\u001b[39;00m\u001b[39m space does not inherit from `gym.spaces.Space`, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(space)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m     79\u001b[0m     check_box_space_fn(space)\n",
      "\u001b[0;31mAssertionError\u001b[0m: action space does not inherit from `gym.spaces.Space`, actual type: <class 'method'>"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"LuxAI_S2-v0\")\n",
    "env = SB3Wrapper(env, zero_bid, place_near_random_ice, controller=SimpleUnitDiscreteController(env.env_cfg))\n",
    "env.reset(seed=9999)\n",
    "img = env.render(\"rgb_array\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class CustomEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        \"\"\"\n",
    "        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.prev_step_metrics = None\n",
    "\n",
    "    def step(self, action):\n",
    "        agent = \"player_0\"\n",
    "        opp_agent = \"player_1\"\n",
    "\n",
    "        opp_factories = self.env.state.factories[opp_agent]\n",
    "        for k in opp_factories.keys():\n",
    "            factory = opp_factories[k]\n",
    "             # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n",
    "            factory.cargo.water = 1000\n",
    "\n",
    "        # submit actions for just one agent to make it single-agent\n",
    "        # and save single-agent versions of the data below\n",
    "        action = {agent: action}\n",
    "        obs, _, done, info = self.env.step(action)\n",
    "        obs = obs[agent]\n",
    "        done = done[agent]\n",
    "        \n",
    "        # we collect stats on teams here. These are useful stats that can be used to help generate reward functions\n",
    "        stats: StatsStateDict = self.env.state.stats[agent]\n",
    "\n",
    "        info = dict()\n",
    "        metrics = dict()\n",
    "        metrics[\"ice_dug\"] = (\n",
    "            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n",
    "        )\n",
    "        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n",
    "\n",
    "        # we save these two to see often the agent updates robot action queues and how often enough\n",
    "        # power to do so and succeed (less frequent updates = more power is saved)\n",
    "        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n",
    "        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n",
    "\n",
    "        # we can save the metrics to info so we can use tensorboard to log them to get a glimpse into how our agent is behaving\n",
    "        info[\"metrics\"] = metrics\n",
    "\n",
    "        reward = 0\n",
    "        if self.prev_step_metrics is not None:\n",
    "            # we check how much ice and water is produced and reward the agent for generating both\n",
    "            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n",
    "            water_produced_this_step = (\n",
    "                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n",
    "            )\n",
    "            # we reward water production more as it is the most important resource for survival\n",
    "            reward = ice_dug_this_step / 100 + water_produced_this_step\n",
    "\n",
    "        self.prev_step_metrics = copy.deepcopy(metrics)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)[\"player_0\"]\n",
    "        self.prev_step_metrics = None\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import TimeLimit\n",
    "def make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=200):\n",
    "    def _init() -> gym.Env:\n",
    "        # verbose = 0\n",
    "        # collect_stats=True lets us track stats like total ice dug during an episode to help create reward functions\n",
    "        # max factories set to 2 for simplification and keeping returns consistent as we survive longer \n",
    "        # if there are more initial resources\n",
    "        env = gym.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n",
    "\n",
    "        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n",
    "        # this will remove the bidding phase and factory placement phase. For factory placement we use\n",
    "        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n",
    "        env = SB3Wrapper(\n",
    "            env,\n",
    "            factory_placement_policy=place_near_random_ice,\n",
    "            controller=SimpleUnitDiscreteController(env.env_cfg),\n",
    "        )\n",
    "        \n",
    "        # changes observation to include a few simple features\n",
    "        env = SimpleUnitObservationWrapper(\n",
    "            env\n",
    "        )\n",
    "        \n",
    "        # convert to single agent, adds our reward\n",
    "        env = CustomEnvWrapper(env)  \n",
    "        \n",
    "        # Add a timelimit to the environment, which can truncate episodes, speed up training\n",
    "        env = TimeLimit(\n",
    "            env, max_episode_steps=max_episode_steps\n",
    "        )\n",
    "        env = Monitor(env) # for SB3 to allow it to record metrics\n",
    "        env.reset(seed=seed + rank)\n",
    "        set_random_seed(seed)\n",
    "        return env\n",
    "\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, tag: str, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.tag = tag\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        c = 0\n",
    "\n",
    "        for i, done in enumerate(self.locals[\"dones\"]):\n",
    "            if done:\n",
    "                info = self.locals[\"infos\"][i]\n",
    "                c += 1\n",
    "                for k in info[\"metrics\"]:\n",
    "                    stat = info[\"metrics\"][k]\n",
    "                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SubprocVecEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m num_envs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m# set max episode steps to 200 for training environments to train faster\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m env \u001b[39m=\u001b[39m SubprocVecEnv([make_env(\u001b[39m\"\u001b[39m\u001b[39mLuxAI_S2-v0\u001b[39m\u001b[39m\"\u001b[39m, i, max_episode_steps\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_envs)])\n\u001b[1;32m     11\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     12\u001b[0m \u001b[39m# set max episode steps to 1000 to match original environment\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SubprocVecEnv' is not defined"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.ppo import PPO\n",
    "\n",
    "set_random_seed(42)\n",
    "log_path = \"logs/exp_1\"\n",
    "num_envs = 1\n",
    "\n",
    "# set max episode steps to 200 for training environments to train faster\n",
    "env = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=200) for i in range(num_envs)])\n",
    "env.reset()\n",
    "# set max episode steps to 1000 to match original environment\n",
    "eval_env = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=1000) for i in range(4)])\n",
    "eval_env.reset()\n",
    "rollout_steps = 4000\n",
    "policy_kwargs = dict(net_arch=(128, 128))\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    n_steps=rollout_steps // num_envs,\n",
    "    batch_size=800,\n",
    "    learning_rate=3e-4,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    n_epochs=2,\n",
    "    target_kl=0.05,\n",
    "    gamma=0.99,\n",
    "    tensorboard_log=osp.join(log_path),\n",
    ")\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=osp.join(log_path, \"models\"),\n",
    "    log_path=osp.join(log_path, \"eval_logs\"),\n",
    "    eval_freq=24_000,\n",
    "    deterministic=False,\n",
    "    render=False,\n",
    "    n_eval_episodes=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m total_timesteps \u001b[39m=\u001b[39m \u001b[39m10_000_000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mlearn(\n\u001b[1;32m      3\u001b[0m     total_timesteps,\n\u001b[1;32m      4\u001b[0m     callback\u001b[39m=\u001b[39m[TensorboardCallback(tag\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_metrics\u001b[39m\u001b[39m\"\u001b[39m), eval_callback],\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39msave(osp\u001b[39m.\u001b[39mjoin(log_path, \u001b[39m\"\u001b[39m\u001b[39mmodels/latest_model\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "total_timesteps = 10_000_000\n",
    "model.learn(\n",
    "    total_timesteps,\n",
    "    callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n",
    ")\n",
    "model.save(osp.join(log_path, \"models/latest_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luxai_s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
