\chapter{Conclusion and Future Work}
\label{ch:conc}

\noindent Multi-agent research plays a crucial role in advancing the pursuit of \textbdd{Artificial General Intelligence} by exploring emergent intelligence in environments that abstract real-world scenarios. These settings, characterized by dynamic agent interactions, diverse interests, and fuzzy goals, \textbdd{mirror the complexity of real-life situations}. Multi-agent competitions, like Lux, serve as invaluable platforms for developers, offering open-source frameworks to simulate diverse environments. These environments enable researchers to explore, test, and refine their ideas, fostering a \textbdd{deeper understanding of communication, cooperation, interaction, and common goal achievement among agents}.

\bigskip

\noindent In this work, we outlined a \textbdd{hybrid} multi-agent control architecture and introduced our \textbdd{trajectory separation} technique for improving its capabilities. We also \textbdd{provided insights} into creating suitable nonlinear approximator architectures (NNs) for policy and value approximation. Additionally, we proposed \textbdd{guidelines for selecting novel RL algorithms} suitable for MARL settings, focusing on the Lux AI environment.

\bigskip

\noindent Regarding the future, we aim to explore the applicability of \textbdd{R-PPO in action queue generation} and \textbdd{QR-DQN as another baseline} for MARL settings. Furthermore, we plan to experiment with \textbdd{continual or multi-phase training} strategies to leverage additional possible strategies within the Lux AI environment. We aim to expand and test our hybrid model on \textbdd{various environments} to assess its generalizability.

\bigskip

\noindent We also intend to explore additional, more advanced techniques beyond the scope of this research. One such approach involves implementing a class system using trajectory splitting and per-class reward shaping. This method will allow us to categorize agents into different types, such as diggers or saboteurs, enhancing the versatility of our models. Furthermore, we plan to integrate an internal communication protocol for agent cooperation, leveraging attention mechanisms (\cite{vaswani2023attention}). Additionally, we aim to improve our models by implementing a transformer decoder-based action-queue generator, further extending their capabilities.

\bigskip

\noindent We are enthusiastic about the future of this project and expect significant potential in extending PPO to multi-agent settings, \textbdd{officially coining the term MA-PPO} (Multi-Agent Proximal Policy Optimization).
